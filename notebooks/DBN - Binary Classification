{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DBN - Binary Classification","provenance":[],"collapsed_sections":["ADjsDA1F8dTv","Dh90Arnk8nlj","dHmezAZD8xP6","e-Zb_37j9D_l","Ij_g-RZqcqFh"],"mount_file_id":"1gRs3ZoZjJcazh3r4rFxj94euZ0u490bN","authorship_tag":"ABX9TyMgQjoC2FKj5vNIR2B4JX2w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["\n","\n","```\n","Author ===> Yagnik Poshiya\n","GitHub ===> @yagnikposhiya\n","Charotar University Of Science and Technology, CSPIT, Changa\n","Anand, Gujarat, India.\n","```\n","\n"],"metadata":{"id":"nlsTU8KGvcbz"}},{"cell_type":"markdown","source":["# **Tensorflow Version**"],"metadata":{"id":"a7FyY7qAVDzr"}},{"cell_type":"markdown","source":["> **NOTE:** In given implementation of **deep belief network** is using **tensorflow 1.x** version."],"metadata":{"id":"Cn_pK_UJF2bU"}},{"cell_type":"code","source":["%tensorflow_version 1.x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"zp7CGCnrVIQ6","executionInfo":{"status":"ok","timestamp":1655015715028,"user_tz":-330,"elapsed":19365,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}},"outputId":"33473c8c-d34f-4955-ea7c-67a8ded7c7b1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","tf.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"eD5pD07NVSuW","executionInfo":{"status":"ok","timestamp":1655015725764,"user_tz":-330,"elapsed":7390,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}},"outputId":"2dd7f5fd-d1ba-4a2b-a2c2-eecea590d904"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.15.2'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["# **Deep Belief Network**"],"metadata":{"id":"34uyu9qQ3j5A"}},{"cell_type":"markdown","source":["## **0. Libraries**"],"metadata":{"id":"k4CyRRPp2l4_"}},{"cell_type":"code","source":["# Libraries for phase 1 and 2\n","import numpy as np\n","from abc import ABCMeta, abstractmethod\n","\n","# Libraries for phase 3\n","from scipy.stats import truncnorm\n","from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, RegressorMixin\n","\n","# Libraries for phase 4\n","import atexit\n","from abc import ABCMeta\n","import tensorflow as tf\n","from sklearn.base import ClassifierMixin, RegressorMixin\n"],"metadata":{"id":"siTRS2JC2lE_","executionInfo":{"status":"ok","timestamp":1655015730742,"user_tz":-330,"elapsed":1177,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## **1. Batch Generator and Integer Category Label Generator**"],"metadata":{"id":"ADjsDA1F8dTv"}},{"cell_type":"code","source":["def batch_generator(batch_size, data, labels=None):\n","    \"\"\"\n","    Generates batches of samples\n","    :param data: array-like, shape = (n_samples, n_features)\n","    :param labels: array-like, shape = (n_samples, )\n","    :return:\n","    \"\"\"\n","    n_batches = int(np.ceil(len(data) / float(batch_size)))\n","    idx = np.random.permutation(len(data))\n","    data_shuffled = data[idx]\n","    if labels is not None:\n","        labels_shuffled = labels[idx]\n","    for i in range(n_batches):\n","        start = i * batch_size\n","        end = start + batch_size\n","        if labels is not None:\n","            yield data_shuffled[start:end, :], labels_shuffled[start:end]\n","        else:\n","            yield data_shuffled[start:end, :]"],"metadata":{"id":"6noM1W3g2uCt","executionInfo":{"status":"ok","timestamp":1655015732027,"user_tz":-330,"elapsed":5,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"vhlbnT178Knv","executionInfo":{"status":"ok","timestamp":1655015734401,"user_tz":-330,"elapsed":3,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"outputs":[],"source":["def to_categorical(labels, num_classes):\n","    \"\"\"\n","    Converts labels as single integer to row vectors. For instance, given a three class problem, labels would be\n","    mapped as label_1: [1 0 0], label_2: [0 1 0], label_3: [0, 0, 1] where labels can be either int or string.\n","    :param labels: array-like, shape = (n_samples, )\n","    :return:\n","    \"\"\"\n","    new_labels = np.zeros([len(labels), num_classes])\n","    label_to_idx_map, idx_to_label_map = dict(), dict()\n","    idx = 0\n","    for i, label in enumerate(labels):\n","        if label not in label_to_idx_map:\n","            label_to_idx_map[label] = idx\n","            idx_to_label_map[idx] = label\n","            idx += 1\n","        new_labels[i][label_to_idx_map[label]] = 1\n","    return new_labels, label_to_idx_map, idx_to_label_map"]},{"cell_type":"markdown","source":["## **2. Activation Functions**"],"metadata":{"id":"Dh90Arnk8nlj"}},{"cell_type":"code","source":["class ActivationFunction(object):\n","    \"\"\"\n","    Class for abstract activation function.\n","    \"\"\"\n","    __metaclass__ = ABCMeta\n","\n","    @abstractmethod\n","    def function(self, x):\n","        return\n","\n","    @abstractmethod\n","    def prime(self, x):\n","        return"],"metadata":{"id":"RDlAjf1R3RkI","executionInfo":{"status":"ok","timestamp":1655015736573,"user_tz":-330,"elapsed":5,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class SigmoidActivationFunction(ActivationFunction):\n","    @classmethod\n","    def function(cls, x):\n","        \"\"\"\n","        Sigmoid function.\n","        :param x: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        return 1 / (1.0 + np.exp(-x))\n","\n","    @classmethod\n","    def prime(cls, x):\n","        \"\"\"\n","        Compute sigmoid first derivative.\n","        :param x: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        return x * (1 - x)"],"metadata":{"id":"PkYR4RdT3VLh","executionInfo":{"status":"ok","timestamp":1655015738473,"user_tz":-330,"elapsed":4,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class ReLUActivationFunction(ActivationFunction):\n","    @classmethod\n","    def function(cls, x):\n","        \"\"\"\n","        Rectified linear function.\n","        :param x: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        return np.maximum(np.zeros(x.shape), x)\n","\n","    @classmethod\n","    def prime(cls, x):\n","        \"\"\"\n","        Rectified linear first derivative.\n","        :param x: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        return (x > 0).astype(int)"],"metadata":{"id":"GtGeepXp3a-Z","executionInfo":{"status":"ok","timestamp":1655015741005,"user_tz":-330,"elapsed":694,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class TanhActivationFunction(ActivationFunction):\n","    @classmethod\n","    def function(cls, x):\n","        \"\"\"\n","        Hyperbolic tangent function.\n","        :param x: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        return np.tanh(x)\n","\n","    @classmethod\n","    def prime(cls, x):\n","        \"\"\"\n","        Hyperbolic tangent first derivative.\n","        :param x: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        return 1 - x * x"],"metadata":{"id":"Y9_tyGAQ8q-_","executionInfo":{"status":"ok","timestamp":1655015742272,"user_tz":-330,"elapsed":3,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## **3. Base models and Implementing General Approach For Regression and Classification Tasks**"],"metadata":{"id":"dHmezAZD8xP6"}},{"cell_type":"code","source":["class BaseModel(object):\n","    def save(self, save_path):\n","        import pickle\n","\n","        with open(save_path, 'wb') as fp:\n","            pickle.dump(self, fp)\n","\n","    @classmethod\n","    def load(cls, load_path):\n","        import pickle\n","\n","        with open(load_path, 'rb') as fp:\n","            return pickle.load(fp)\n"],"metadata":{"id":"MGb2FpUI4Q3O","executionInfo":{"status":"ok","timestamp":1655015746470,"user_tz":-330,"elapsed":826,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class BinaryRBM(BaseEstimator, TransformerMixin, BaseModel):\n","    \"\"\"\n","    This class implements a Binary Restricted Boltzmann machine.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 n_hidden_units=100,\n","                 activation_function='sigmoid',\n","                 optimization_algorithm='sgd',\n","                 learning_rate=1e-3,\n","                 n_epochs=10,\n","                 contrastive_divergence_iter=1,\n","                 batch_size=32,\n","                 verbose=True):\n","        self.n_hidden_units = n_hidden_units\n","        self.activation_function = activation_function\n","        self.optimization_algorithm = optimization_algorithm\n","        self.learning_rate = learning_rate\n","        self.n_epochs = n_epochs\n","        self.contrastive_divergence_iter = contrastive_divergence_iter\n","        self.batch_size = batch_size\n","        self.verbose = verbose\n","\n","    def fit(self, X):\n","        \"\"\"\n","        Fit a model given data.\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        # Initialize RBM parameters\n","        self.n_visible_units = X.shape[1]\n","        if self.activation_function == 'sigmoid':\n","            self.W = np.random.randn(self.n_hidden_units, self.n_visible_units) / np.sqrt(self.n_visible_units)\n","            self.c = np.random.randn(self.n_hidden_units) / np.sqrt(self.n_visible_units)\n","            self.b = np.random.randn(self.n_visible_units) / np.sqrt(self.n_visible_units)\n","            self._activation_function_class = SigmoidActivationFunction\n","        elif self.activation_function == 'relu':\n","            self.W = truncnorm.rvs(-0.2, 0.2, size=[self.n_hidden_units, self.n_visible_units]) / np.sqrt(\n","                self.n_visible_units)\n","            self.c = np.full(self.n_hidden_units, 0.1) / np.sqrt(self.n_visible_units)\n","            self.b = np.full(self.n_visible_units, 0.1) / np.sqrt(self.n_visible_units)\n","            self._activation_function_class = ReLUActivationFunction\n","        else:\n","            raise ValueError(\"Invalid activation function.\")\n","\n","        if self.optimization_algorithm == 'sgd':\n","            self._stochastic_gradient_descent(X)\n","        else:\n","            raise ValueError(\"Invalid optimization algorithm.\")\n","        return self\n","\n","    def transform(self, X):\n","        \"\"\"\n","        Transforms data using the fitted model.\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        if len(X.shape) == 1:  # It is a single sample\n","            return self._compute_hidden_units(X)\n","        transformed_data = self._compute_hidden_units_matrix(X)\n","        return transformed_data\n","\n","    def _reconstruct(self, transformed_data):\n","        \"\"\"\n","        Reconstruct visible units given the hidden layer output.\n","        :param transformed_data: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        return self._compute_visible_units_matrix(transformed_data)\n","\n","    def _stochastic_gradient_descent(self, _data):\n","        \"\"\"\n","        Performs stochastic gradient descend optimization algorithm.\n","        :param _data: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        accum_delta_W = np.zeros(self.W.shape)\n","        accum_delta_b = np.zeros(self.b.shape)\n","        accum_delta_c = np.zeros(self.c.shape)\n","        for iteration in range(1, self.n_epochs + 1):\n","            idx = np.random.permutation(len(_data))\n","            data = _data[idx]\n","            for batch in batch_generator(self.batch_size, data):\n","                accum_delta_W[:] = .0\n","                accum_delta_b[:] = .0\n","                accum_delta_c[:] = .0\n","                for sample in batch:\n","                    delta_W, delta_b, delta_c = self._contrastive_divergence(sample)\n","                    accum_delta_W += delta_W\n","                    accum_delta_b += delta_b\n","                    accum_delta_c += delta_c\n","                self.W += self.learning_rate * (accum_delta_W / self.batch_size)\n","                self.b += self.learning_rate * (accum_delta_b / self.batch_size)\n","                self.c += self.learning_rate * (accum_delta_c / self.batch_size)\n","            if self.verbose:\n","                error = self._compute_reconstruction_error(data)\n","                print(\">> Epoch %d finished \\tRBM Reconstruction error %f\" % (iteration, error))\n","\n","    def _contrastive_divergence(self, vector_visible_units):\n","        \"\"\"\n","        Computes gradients using Contrastive Divergence method.\n","        :param vector_visible_units: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        v_0 = vector_visible_units\n","        v_t = np.array(v_0)\n","\n","        # Sampling\n","        for t in range(self.contrastive_divergence_iter):\n","            h_t = self._sample_hidden_units(v_t)\n","            v_t = self._compute_visible_units(h_t)\n","\n","        # Computing deltas\n","        v_k = v_t\n","        h_0 = self._compute_hidden_units(v_0)\n","        h_k = self._compute_hidden_units(v_k)\n","        delta_W = np.outer(h_0, v_0) - np.outer(h_k, v_k)\n","        delta_b = v_0 - v_k\n","        delta_c = h_0 - h_k\n","\n","        return delta_W, delta_b, delta_c\n","\n","    def _sample_hidden_units(self, vector_visible_units):\n","        \"\"\"\n","        Computes hidden unit activations by sampling from a binomial distribution.\n","        :param vector_visible_units: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        hidden_units = self._compute_hidden_units(vector_visible_units)\n","        return (np.random.random_sample(len(hidden_units)) < hidden_units).astype(np.int64)\n","\n","    def _sample_visible_units(self, vector_hidden_units):\n","        \"\"\"\n","        Computes visible unit activations by sampling from a binomial distribution.\n","        :param vector_hidden_units: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        visible_units = self._compute_visible_units(vector_hidden_units)\n","        return (np.random.random_sample(len(visible_units)) < visible_units).astype(np.int64)\n","\n","    def _compute_hidden_units(self, vector_visible_units):\n","        \"\"\"\n","        Computes hidden unit outputs.\n","        :param vector_visible_units: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        v = np.expand_dims(vector_visible_units, 0)\n","        h = np.squeeze(self._compute_hidden_units_matrix(v))\n","        return np.array([h]) if not h.shape else h\n","\n","    def _compute_hidden_units_matrix(self, matrix_visible_units):\n","        \"\"\"\n","        Computes hidden unit outputs.\n","        :param matrix_visible_units: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        return np.transpose(self._activation_function_class.function(\n","            np.dot(self.W, np.transpose(matrix_visible_units)) + self.c[:, np.newaxis]))\n","\n","    def _compute_visible_units(self, vector_hidden_units):\n","        \"\"\"\n","        Computes visible (or input) unit outputs.\n","        :param vector_hidden_units: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        h = np.expand_dims(vector_hidden_units, 0)\n","        v = np.squeeze(self._compute_visible_units_matrix(h))\n","        return np.array([v]) if not v.shape else v\n","\n","    def _compute_visible_units_matrix(self, matrix_hidden_units):\n","        \"\"\"\n","        Computes visible (or input) unit outputs.\n","        :param matrix_hidden_units: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        return self._activation_function_class.function(np.dot(matrix_hidden_units, self.W) + self.b[np.newaxis, :])\n","\n","    def _compute_free_energy(self, vector_visible_units):\n","        \"\"\"\n","        Computes the RBM free energy.\n","        :param vector_visible_units: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        v = vector_visible_units\n","        return - np.dot(self.b, v) - np.sum(np.log(1 + np.exp(np.dot(self.W, v) + self.c)))\n","\n","    def _compute_reconstruction_error(self, data):\n","        \"\"\"\n","        Computes the reconstruction error of the data.\n","        :param data: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        data_transformed = self.transform(data)\n","        data_reconstructed = self._reconstruct(data_transformed)\n","        return np.mean(np.sum((data_reconstructed - data) ** 2, 1))\n"],"metadata":{"id":"cv7lHSy54gBw","executionInfo":{"status":"ok","timestamp":1655015748254,"user_tz":-330,"elapsed":6,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["class UnsupervisedDBN(BaseEstimator, TransformerMixin, BaseModel):\n","    \"\"\"\n","    This class implements a unsupervised Deep Belief Network.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 hidden_layers_structure=[100, 100],\n","                 activation_function='sigmoid',\n","                 optimization_algorithm='sgd',\n","                 learning_rate_rbm=1e-3,\n","                 n_epochs_rbm=10,\n","                 contrastive_divergence_iter=1,\n","                 batch_size=32,\n","                 verbose=True):\n","        self.hidden_layers_structure = hidden_layers_structure\n","        self.activation_function = activation_function\n","        self.optimization_algorithm = optimization_algorithm\n","        self.learning_rate_rbm = learning_rate_rbm\n","        self.n_epochs_rbm = n_epochs_rbm\n","        self.contrastive_divergence_iter = contrastive_divergence_iter\n","        self.batch_size = batch_size\n","        self.rbm_layers = None\n","        self.verbose = verbose\n","        self.rbm_class = BinaryRBM\n","\n","    def fit(self, X, y=None):\n","        \"\"\"\n","        Fits a model given data.\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        # Initialize rbm layers\n","        self.rbm_layers = list()\n","        for n_hidden_units in self.hidden_layers_structure:\n","            rbm = self.rbm_class(n_hidden_units=n_hidden_units,\n","                                 activation_function=self.activation_function,\n","                                 optimization_algorithm=self.optimization_algorithm,\n","                                 learning_rate=self.learning_rate_rbm,\n","                                 n_epochs=self.n_epochs_rbm,\n","                                 contrastive_divergence_iter=self.contrastive_divergence_iter,\n","                                 batch_size=self.batch_size,\n","                                 verbose=self.verbose)\n","            self.rbm_layers.append(rbm)\n","\n","        # Fit RBM\n","        if self.verbose:\n","            print(\"[START] Pre-training step:\")\n","        input_data = X\n","        for rbm in self.rbm_layers:\n","            rbm.fit(input_data)\n","            input_data = rbm.transform(input_data)\n","        if self.verbose:\n","            print(\"[END] Pre-training step\")\n","        return self\n","\n","    def transform(self, X):\n","        \"\"\"\n","        Transforms data using the fitted model.\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        input_data = X\n","        for rbm in self.rbm_layers:\n","            input_data = rbm.transform(input_data)\n","        return input_data"],"metadata":{"id":"72u4scmB4xRP","executionInfo":{"status":"ok","timestamp":1655015756301,"user_tz":-330,"elapsed":631,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class AbstractSupervisedDBN(BaseEstimator, BaseModel):\n","    \"\"\"\n","    Abstract class for supervised Deep Belief Network.\n","    \"\"\"\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self,\n","                 unsupervised_dbn_class,\n","                 hidden_layers_structure=[100, 100],\n","                 activation_function='sigmoid',\n","                 optimization_algorithm='sgd',\n","                 learning_rate=1e-3,\n","                 learning_rate_rbm=1e-3,\n","                 n_iter_backprop=100,\n","                 l2_regularization=1.0,\n","                 n_epochs_rbm=10,\n","                 contrastive_divergence_iter=1,\n","                 batch_size=32,\n","                 dropout_p=0,  # float between 0 and 1. Fraction of the input units to drop\n","                 verbose=True):\n","        self.unsupervised_dbn = unsupervised_dbn_class(hidden_layers_structure=hidden_layers_structure,\n","                                                       activation_function=activation_function,\n","                                                       optimization_algorithm=optimization_algorithm,\n","                                                       learning_rate_rbm=learning_rate_rbm,\n","                                                       n_epochs_rbm=n_epochs_rbm,\n","                                                       contrastive_divergence_iter=contrastive_divergence_iter,\n","                                                       batch_size=batch_size,\n","                                                       verbose=verbose)\n","        self.unsupervised_dbn_class = unsupervised_dbn_class\n","        self.n_iter_backprop = n_iter_backprop\n","        self.l2_regularization = l2_regularization\n","        self.learning_rate = learning_rate\n","        self.batch_size = batch_size\n","        self.dropout_p = dropout_p\n","        self.p = 1 - self.dropout_p\n","        self.verbose = verbose\n","\n","    def fit(self, X, y=None, pre_train=True):\n","        \"\"\"\n","        Fits a model given data.\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :param y : array-like, shape = (n_samples, )\n","        :param pre_train: bool\n","        :return:\n","        \"\"\"\n","        if pre_train:\n","            self.pre_train(X)\n","        self._fine_tuning(X, y)\n","        return self\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predicts the target given data.\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        if len(X.shape) == 1:  # It is a single sample\n","            X = np.expand_dims(X, 0)\n","        transformed_data = self.transform(X)\n","        predicted_data = self._compute_output_units_matrix(transformed_data)\n","        return predicted_data\n","\n","    def pre_train(self, X):\n","        \"\"\"\n","        Apply unsupervised network pre-training.\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        self.unsupervised_dbn.fit(X)\n","        return self\n","\n","    def transform(self, *args):\n","        return self.unsupervised_dbn.transform(*args)\n","\n","    @abstractmethod\n","    def _transform_labels_to_network_format(self, labels):\n","        return\n","\n","    @abstractmethod\n","    def _compute_output_units_matrix(self, matrix_visible_units):\n","        return\n","\n","    @abstractmethod\n","    def _determine_num_output_neurons(self, labels):\n","        return\n","\n","    @abstractmethod\n","    def _stochastic_gradient_descent(self, data, labels):\n","        return\n","\n","    @abstractmethod\n","    def _fine_tuning(self, data, _labels):\n","        return\n"],"metadata":{"id":"BEWl8j30450I","executionInfo":{"status":"ok","timestamp":1655015760385,"user_tz":-330,"elapsed":617,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class NumPyAbstractSupervisedDBN(AbstractSupervisedDBN):\n","    \"\"\"\n","    Abstract class for supervised Deep Belief Network in NumPy\n","    \"\"\"\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self, **kwargs):\n","        super(NumPyAbstractSupervisedDBN, self).__init__(UnsupervisedDBN, **kwargs)\n","\n","    def _compute_activations(self, sample):\n","        \"\"\"\n","        Compute output values of all layers.\n","        :param sample: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        input_data = sample\n","        if self.dropout_p > 0:\n","            r = np.random.binomial(1, self.p, len(input_data))\n","            input_data *= r\n","        layers_activation = list()\n","\n","        for rbm in self.unsupervised_dbn.rbm_layers:\n","            input_data = rbm.transform(input_data)\n","            if self.dropout_p > 0:\n","                r = np.random.binomial(1, self.p, len(input_data))\n","                input_data *= r\n","            layers_activation.append(input_data)\n","\n","        # Computing activation of output layer\n","        input_data = self._compute_output_units(input_data)\n","        layers_activation.append(input_data)\n","\n","        return layers_activation\n","\n","    def _stochastic_gradient_descent(self, _data, _labels):\n","        \"\"\"\n","        Performs stochastic gradient descend optimization algorithm.\n","        :param _data: array-like, shape = (n_samples, n_features)\n","        :param _labels: array-like, shape = (n_samples, targets)\n","        :return:\n","        \"\"\"\n","        if self.verbose:\n","            matrix_error = np.zeros([len(_data), self.num_classes])\n","        num_samples = len(_data)\n","        accum_delta_W = [np.zeros(rbm.W.shape) for rbm in self.unsupervised_dbn.rbm_layers]\n","        accum_delta_W.append(np.zeros(self.W.shape))\n","        accum_delta_bias = [np.zeros(rbm.c.shape) for rbm in self.unsupervised_dbn.rbm_layers]\n","        accum_delta_bias.append(np.zeros(self.b.shape))\n","\n","        for iteration in range(1, self.n_iter_backprop + 1):\n","            idx = np.random.permutation(len(_data))\n","            data = _data[idx]\n","            labels = _labels[idx]\n","            i = 0\n","            for batch_data, batch_labels in batch_generator(self.batch_size, data, labels):\n","                # Clear arrays\n","                for arr1, arr2 in zip(accum_delta_W, accum_delta_bias):\n","                    arr1[:], arr2[:] = .0, .0\n","                for sample, label in zip(batch_data, batch_labels):\n","                    delta_W, delta_bias, predicted = self._backpropagation(sample, label)\n","                    for layer in range(len(self.unsupervised_dbn.rbm_layers) + 1):\n","                        accum_delta_W[layer] += delta_W[layer]\n","                        accum_delta_bias[layer] += delta_bias[layer]\n","                    if self.verbose:\n","                        loss = self._compute_loss(predicted, label)\n","                        matrix_error[i, :] = loss\n","                        i += 1\n","\n","                layer = 0\n","                for rbm in self.unsupervised_dbn.rbm_layers:\n","                    # Updating parameters of hidden layers\n","                    rbm.W = (1 - (\n","                        self.learning_rate * self.l2_regularization) / num_samples) * rbm.W - self.learning_rate * (\n","                        accum_delta_W[layer] / self.batch_size)\n","                    rbm.c -= self.learning_rate * (accum_delta_bias[layer] / self.batch_size)\n","                    layer += 1\n","                # Updating parameters of output layer\n","                self.W = (1 - (\n","                    self.learning_rate * self.l2_regularization) / num_samples) * self.W - self.learning_rate * (\n","                    accum_delta_W[layer] / self.batch_size)\n","                self.b -= self.learning_rate * (accum_delta_bias[layer] / self.batch_size)\n","\n","            if self.verbose:\n","                error = np.mean(np.sum(matrix_error, 1))\n","                print(\">> Epoch %d finished \\tANN training loss %f\" % (iteration, error))\n","\n","    def _backpropagation(self, input_vector, label):\n","        \"\"\"\n","        Performs Backpropagation algorithm for computing gradients.\n","        :param input_vector: array-like, shape = (n_features, )\n","        :param label: array-like, shape = (n_targets, )\n","        :return:\n","        \"\"\"\n","        x, y = input_vector, label\n","        deltas = list()\n","        list_layer_weights = list()\n","        for rbm in self.unsupervised_dbn.rbm_layers:\n","            list_layer_weights.append(rbm.W)\n","        list_layer_weights.append(self.W)\n","\n","        # Forward pass\n","        layers_activation = self._compute_activations(input_vector)\n","\n","        # Backward pass: computing deltas\n","        activation_output_layer = layers_activation[-1]\n","        delta_output_layer = self._compute_output_layer_delta(y, activation_output_layer)\n","        deltas.append(delta_output_layer)\n","        layer_idx = list(range(len(self.unsupervised_dbn.rbm_layers)))\n","        layer_idx.reverse()\n","        delta_previous_layer = delta_output_layer\n","        for layer in layer_idx:\n","            neuron_activations = layers_activation[layer]\n","            W = list_layer_weights[layer + 1]\n","            delta = np.dot(delta_previous_layer, W) * self.unsupervised_dbn.rbm_layers[\n","                layer]._activation_function_class.prime(neuron_activations)\n","            deltas.append(delta)\n","            delta_previous_layer = delta\n","        deltas.reverse()\n","\n","        # Computing gradients\n","        layers_activation.pop()\n","        layers_activation.insert(0, input_vector)\n","        layer_gradient_weights, layer_gradient_bias = list(), list()\n","        for layer in range(len(list_layer_weights)):\n","            neuron_activations = layers_activation[layer]\n","            delta = deltas[layer]\n","            gradient_W = np.outer(delta, neuron_activations)\n","            layer_gradient_weights.append(gradient_W)\n","            layer_gradient_bias.append(delta)\n","\n","        return layer_gradient_weights, layer_gradient_bias, activation_output_layer\n","\n","    def _fine_tuning(self, data, _labels):\n","        \"\"\"\n","        Entry point of the fine tuning procedure.\n","        :param data: array-like, shape = (n_samples, n_features)\n","        :param _labels: array-like, shape = (n_samples, targets)\n","        :return:\n","        \"\"\"\n","        self.num_classes = self._determine_num_output_neurons(_labels)\n","        n_hidden_units_previous_layer = self.unsupervised_dbn.rbm_layers[-1].n_hidden_units\n","        self.W = np.random.randn(self.num_classes, n_hidden_units_previous_layer) / np.sqrt(\n","            n_hidden_units_previous_layer)\n","        self.b = np.random.randn(self.num_classes) / np.sqrt(n_hidden_units_previous_layer)\n","\n","        labels = self._transform_labels_to_network_format(_labels)\n","\n","        # Scaling up weights obtained from pretraining\n","        for rbm in self.unsupervised_dbn.rbm_layers:\n","            rbm.W /= self.p\n","            rbm.c /= self.p\n","\n","        if self.verbose:\n","            print(\"[START] Fine tuning step:\")\n","\n","        if self.unsupervised_dbn.optimization_algorithm == 'sgd':\n","            self._stochastic_gradient_descent(data, labels)\n","        else:\n","            raise ValueError(\"Invalid optimization algorithm.\")\n","\n","        # Scaling down weights obtained from pretraining\n","        for rbm in self.unsupervised_dbn.rbm_layers:\n","            rbm.W *= self.p\n","            rbm.c *= self.p\n","\n","        if self.verbose:\n","            print(\"[END] Fine tuning step\")\n","\n","    @abstractmethod\n","    def _compute_loss(self, predicted, label):\n","        return\n","\n","    @abstractmethod\n","    def _compute_output_layer_delta(self, label, predicted):\n","        return\n"],"metadata":{"id":"-JqbpGny5BTe","executionInfo":{"status":"ok","timestamp":1655015764992,"user_tz":-330,"elapsed":515,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["> **NOTE:** class **SupervisedDBNClassification** is used here to classify fingerprint images into **Real** and **Fake** category."],"metadata":{"id":"G6PADd1T5S2z"}},{"cell_type":"code","source":["class SupervisedDBNClassification(NumPyAbstractSupervisedDBN, ClassifierMixin):\n","    \"\"\"\n","    This class implements a Deep Belief Network for classification problems.\n","    It appends a Softmax Linear Classifier as output layer.\n","    \"\"\"\n","\n","    def _transform_labels_to_network_format(self, labels):\n","        \"\"\"\n","        Converts labels as single integer to row vectors. For instance, given a three class problem, labels would be\n","        mapped as label_1: [1 0 0], label_2: [0 1 0], label_3: [0, 0, 1] where labels can be either int or string.\n","        :param labels: array-like, shape = (n_samples, )\n","        :return:\n","        \"\"\"\n","        new_labels = np.zeros([len(labels), self.num_classes])\n","        self.label_to_idx_map, self.idx_to_label_map = dict(), dict()\n","        idx = 0\n","        for i, label in enumerate(labels):\n","            if label not in self.label_to_idx_map:\n","                self.label_to_idx_map[label] = idx\n","                self.idx_to_label_map[idx] = label\n","                idx += 1\n","            new_labels[i][self.label_to_idx_map[label]] = 1\n","        return new_labels\n","\n","    def _transform_network_format_to_labels(self, indexes):\n","        \"\"\"\n","        Converts network output to original labels.\n","        :param indexes: array-like, shape = (n_samples, )\n","        :return:\n","        \"\"\"\n","        return list(map(lambda idx: self.idx_to_label_map[idx], indexes))\n","\n","    def _compute_output_units(self, vector_visible_units):\n","        \"\"\"\n","        Compute activations of output units.\n","        :param vector_visible_units: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        v = vector_visible_units\n","        scores = np.dot(self.W, v) + self.b\n","        # get unnormalized probabilities\n","        exp_scores = np.exp(scores)\n","        # normalize them for each example\n","        return exp_scores / np.sum(exp_scores)\n","\n","    def _compute_output_units_matrix(self, matrix_visible_units):\n","        \"\"\"\n","        Compute activations of output units.\n","        :param matrix_visible_units: shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        matrix_scores = np.transpose(np.dot(self.W, np.transpose(matrix_visible_units)) + self.b[:, np.newaxis])\n","        exp_scores = np.exp(matrix_scores)\n","        return exp_scores / np.expand_dims(np.sum(exp_scores, axis=1), 1)\n","\n","    def _compute_output_layer_delta(self, label, predicted):\n","        \"\"\"\n","        Compute deltas of the output layer, using cross-entropy cost function.\n","        :param label: array-like, shape = (n_features, )\n","        :param predicted: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        dscores = np.array(predicted)\n","        dscores[np.where(label == 1)] -= 1\n","        return dscores\n","\n","    def predict_proba(self, X):\n","        \"\"\"\n","        Predicts probability distribution of classes for each sample in the given data.\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        return super(SupervisedDBNClassification, self).predict(X)\n","\n","    def predict_proba_dict(self, X):\n","        \"\"\"\n","        Predicts probability distribution of classes for each sample in the given data.\n","        Returns a list of dictionaries, one per sample. Each dict contains {label_1: prob_1, ..., label_j: prob_j}\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        if len(X.shape) == 1:  # It is a single sample\n","            X = np.expand_dims(X, 0)\n","\n","        predicted_probs = self.predict_proba(X)\n","\n","        result = []\n","        num_of_data, num_of_labels = predicted_probs.shape\n","        for i in range(num_of_data):\n","            # key : label\n","            # value : predicted probability\n","            dict_prob = {}\n","            for j in range(num_of_labels):\n","                dict_prob[self.idx_to_label_map[j]] = predicted_probs[i][j]\n","            result.append(dict_prob)\n","\n","        return result\n","\n","    def predict(self, X):\n","        probs = self.predict_proba(X)\n","        indexes = np.argmax(probs, axis=1)\n","        return self._transform_network_format_to_labels(indexes)\n","\n","    def _determine_num_output_neurons(self, labels):\n","        \"\"\"\n","        Given labels, compute the needed number of output units.\n","        :param labels: shape = (n_samples, )\n","        :return:\n","        \"\"\"\n","        return len(np.unique(labels))\n","\n","    def _compute_loss(self, probs, label):\n","        \"\"\"\n","        Computes categorical cross-entropy loss\n","        :param probs:\n","        :param label:\n","        :return:\n","        \"\"\"\n","        return -np.log(probs[np.where(label == 1)])"],"metadata":{"id":"wCkvmqYV5Ku-","executionInfo":{"status":"ok","timestamp":1655015772473,"user_tz":-330,"elapsed":515,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["class SupervisedDBNRegression(NumPyAbstractSupervisedDBN, RegressorMixin):\n","    \"\"\"\n","    This class implements a Deep Belief Network for regression problems.\n","    \"\"\"\n","\n","    def _transform_labels_to_network_format(self, labels):\n","        \"\"\"\n","        Returns the same labels since regression case does not need to convert anything.\n","        :param labels: array-like, shape = (n_samples, targets)\n","        :return:\n","        \"\"\"\n","        return labels\n","\n","    def _compute_output_units(self, vector_visible_units):\n","        \"\"\"\n","        Compute activations of output units.\n","        :param vector_visible_units: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        v = vector_visible_units\n","        return np.dot(self.W, v) + self.b\n","\n","    def _compute_output_units_matrix(self, matrix_visible_units):\n","        \"\"\"\n","        Compute activations of output units.\n","        :param matrix_visible_units: shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        return np.transpose(np.dot(self.W, np.transpose(matrix_visible_units)) + self.b[:, np.newaxis])\n","\n","    def _compute_output_layer_delta(self, label, predicted):\n","        \"\"\"\n","        Compute deltas of the output layer for the regression case, using common (one-half) squared-error cost function.\n","        :param label: array-like, shape = (n_features, )\n","        :param predicted: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        return -(label - predicted)\n","\n","    def _determine_num_output_neurons(self, labels):\n","        \"\"\"\n","        Given labels, compute the needed number of output units.\n","        :param labels: shape = (n_samples, n_targets)\n","        :return:\n","        \"\"\"\n","        if len(labels.shape) == 1:\n","            return 1\n","        else:\n","            return labels.shape[1]\n","\n","    def _compute_loss(self, predicted, label):\n","        \"\"\"\n","        Computes Mean squared error loss.\n","        :param predicted:\n","        :param label:\n","        :return:\n","        \"\"\"\n","        error = predicted - label\n","        return error * error"],"metadata":{"id":"rQAripct8zoo","executionInfo":{"status":"ok","timestamp":1655015778202,"user_tz":-330,"elapsed":516,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## **4. Defining Models With Their Performance Measure Functionality**"],"metadata":{"id":"e-Zb_37j9D_l"}},{"cell_type":"code","source":["\"\"\" Giving alias to class name  \"\"\"\n","\n","BaseBinaryRBM = BinaryRBM\n","BaseAbstractSupervisedDBN = AbstractSupervisedDBN\n","BaseUnsupervisedDBN = UnsupervisedDBN"],"metadata":{"id":"2G5v5Jv9RXJB","executionInfo":{"status":"ok","timestamp":1655015782413,"user_tz":-330,"elapsed":386,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def close_session():\n","    sess.close()\n","\n","\n","sess = tf.Session()\n","atexit.register(close_session)\n","\n","\n","def weight_variable(func, shape, stddev, dtype=tf.float32):\n","    initial = func(shape, stddev=stddev, dtype=dtype)\n","    return tf.Variable(initial)\n","\n","\n","def bias_variable(value, shape, dtype=tf.float32):\n","    initial = tf.constant(value, shape=shape, dtype=dtype)\n","    return tf.Variable(initial)"],"metadata":{"id":"sKf2OmLr7mBE","executionInfo":{"status":"ok","timestamp":1655015785199,"user_tz":-330,"elapsed":684,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["class BaseTensorFlowModel(BaseModel):\n","    def save(self, save_path):\n","        import pickle\n","\n","        with open(save_path, 'wb') as fp:\n","            pickle.dump(self.to_dict(), fp)\n","\n","    @classmethod\n","    def load(cls, load_path):\n","        import pickle\n","\n","        with open(load_path, 'rb') as fp:\n","            dct_to_load = pickle.load(fp)\n","            return cls.from_dict(dct_to_load)\n","\n","    def to_dict(self):\n","        dct_to_save = {name: self.__getattribute__(name) for name in self._get_param_names()}\n","        dct_to_save.update(\n","            {name: self.__getattribute__(name).eval(sess) for name in self._get_weight_variables_names()})\n","        return dct_to_save\n","\n","    @classmethod\n","    def from_dict(cls, dct_to_load):\n","        pass\n","\n","    def _build_model(self, weights=None):\n","        pass\n","\n","    def _initialize_weights(self, weights):\n","        pass\n","\n","    @classmethod\n","    def _get_weight_variables_names(cls):\n","        pass\n","\n","    @classmethod\n","    def _get_param_names(cls):\n","        pass"],"metadata":{"id":"0xXdpqKy7r1Q","executionInfo":{"status":"ok","timestamp":1655015789465,"user_tz":-330,"elapsed":515,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["class BinaryRBM(BaseBinaryRBM, BaseTensorFlowModel):\n","    \"\"\"\n","    This class implements a Binary Restricted Boltzmann machine based on TensorFlow.\n","    \"\"\"\n","\n","    def fit(self, X):\n","        \"\"\"\n","        Fit a model given data.\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        self.n_visible_units = X.shape[1]\n","\n","        # Initialize RBM parameters\n","        self._build_model()\n","\n","        sess.run(tf.variables_initializer([self.W, self.c, self.b]))\n","\n","        if self.optimization_algorithm == 'sgd':\n","            self._stochastic_gradient_descent(X)\n","        else:\n","            raise ValueError(\"Invalid optimization algorithm.\")\n","        return\n","\n","    @classmethod\n","    def _get_weight_variables_names(cls):\n","        return ['W', 'c', 'b']\n","\n","    @classmethod\n","    def _get_param_names(cls):\n","        return ['n_hidden_units',\n","                'n_visible_units',\n","                'activation_function',\n","                'optimization_algorithm',\n","                'learning_rate',\n","                'n_epochs',\n","                'contrastive_divergence_iter',\n","                'batch_size',\n","                'verbose',\n","                '_activation_function_class']\n","\n","    def _initialize_weights(self, weights):\n","        if weights:\n","            for attr_name, value in weights.items():\n","                self.__setattr__(attr_name, tf.Variable(value))\n","        else:\n","            if self.activation_function == 'sigmoid':\n","                stddev = 1.0 / np.sqrt(self.n_visible_units)\n","                self.W = weight_variable(tf.random_normal, [self.n_hidden_units, self.n_visible_units], stddev)\n","                self.c = weight_variable(tf.random_normal, [self.n_hidden_units], stddev)\n","                self.b = weight_variable(tf.random_normal, [self.n_visible_units], stddev)\n","                self._activation_function_class = tf.nn.sigmoid\n","            elif self.activation_function == 'relu':\n","                stddev = 0.1 / np.sqrt(self.n_visible_units)\n","                self.W = weight_variable(tf.truncated_normal, [self.n_hidden_units, self.n_visible_units], stddev)\n","                self.c = bias_variable(stddev, [self.n_hidden_units])\n","                self.b = bias_variable(stddev, [self.n_visible_units])\n","                self._activation_function_class = tf.nn.relu\n","            else:\n","                raise ValueError(\"Invalid activation function.\")\n","\n","    def _build_model(self, weights=None):\n","        \"\"\"\n","        Builds TensorFlow model.\n","        :return:\n","        \"\"\"\n","        # initialize weights and biases\n","        self._initialize_weights(weights)\n","\n","        # TensorFlow operations\n","        self.visible_units_placeholder = tf.placeholder(tf.float32, shape=[None, self.n_visible_units])\n","        self.compute_hidden_units_op = self._activation_function_class(\n","            tf.transpose(tf.matmul(self.W, tf.transpose(self.visible_units_placeholder))) + self.c)\n","        self.hidden_units_placeholder = tf.placeholder(tf.float32, shape=[None, self.n_hidden_units])\n","        self.compute_visible_units_op = self._activation_function_class(\n","            tf.matmul(self.hidden_units_placeholder, self.W) + self.b)\n","        self.random_uniform_values = tf.Variable(tf.random_uniform([self.batch_size, self.n_hidden_units]))\n","        sample_hidden_units_op = tf.to_float(self.random_uniform_values < self.compute_hidden_units_op)\n","        self.random_variables = [self.random_uniform_values]\n","\n","        # Positive gradient\n","        # Outer product. N is the batch size length.\n","        # From http://stackoverflow.com/questions/35213787/tensorflow-batch-outer-product\n","        positive_gradient_op = tf.matmul(tf.expand_dims(sample_hidden_units_op, 2),  # [N, U, 1]\n","                                         tf.expand_dims(self.visible_units_placeholder, 1))  # [N, 1, V]\n","\n","        # Negative gradient\n","        # Gibbs sampling\n","        sample_hidden_units_gibbs_step_op = sample_hidden_units_op\n","        for t in range(self.contrastive_divergence_iter):\n","            compute_visible_units_op = self._activation_function_class(\n","                tf.matmul(sample_hidden_units_gibbs_step_op, self.W) + self.b)\n","            compute_hidden_units_gibbs_step_op = self._activation_function_class(\n","                tf.transpose(tf.matmul(self.W, tf.transpose(compute_visible_units_op))) + self.c)\n","            random_uniform_values = tf.Variable(tf.random_uniform([self.batch_size, self.n_hidden_units]))\n","            sample_hidden_units_gibbs_step_op = tf.to_float(random_uniform_values < compute_hidden_units_gibbs_step_op)\n","            self.random_variables.append(random_uniform_values)\n","\n","        negative_gradient_op = tf.matmul(tf.expand_dims(sample_hidden_units_gibbs_step_op, 2),  # [N, U, 1]\n","                                         tf.expand_dims(compute_visible_units_op, 1))  # [N, 1, V]\n","\n","        compute_delta_W = tf.reduce_mean(positive_gradient_op - negative_gradient_op, 0)\n","        compute_delta_b = tf.reduce_mean(self.visible_units_placeholder - compute_visible_units_op, 0)\n","        compute_delta_c = tf.reduce_mean(sample_hidden_units_op - sample_hidden_units_gibbs_step_op, 0)\n","\n","        self.update_W = tf.assign_add(self.W, self.learning_rate * compute_delta_W)\n","        self.update_b = tf.assign_add(self.b, self.learning_rate * compute_delta_b)\n","        self.update_c = tf.assign_add(self.c, self.learning_rate * compute_delta_c)\n","\n","    @classmethod\n","    def from_dict(cls, dct_to_load):\n","        weights = {var_name: dct_to_load.pop(var_name) for var_name in cls._get_weight_variables_names()}\n","\n","        _activation_function_class = dct_to_load.pop('_activation_function_class')\n","        n_visible_units = dct_to_load.pop('n_visible_units')\n","\n","        instance = cls(**dct_to_load)\n","        setattr(instance, '_activation_function_class', _activation_function_class)\n","        setattr(instance, 'n_visible_units', n_visible_units)\n","\n","        # Initialize RBM parameters\n","        instance._build_model(weights)\n","        sess.run(tf.variables_initializer([getattr(instance, name) for name in cls._get_weight_variables_names()]))\n","\n","        return instance\n","\n","    def _stochastic_gradient_descent(self, _data):\n","        \"\"\"\n","        Performs stochastic gradient descend optimization algorithm.\n","        :param _data: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        for iteration in range(1, self.n_epochs + 1):\n","            idx = np.random.permutation(len(_data))\n","            data = _data[idx]\n","            for batch in batch_generator(self.batch_size, data):\n","                if len(batch) < self.batch_size:\n","                    # Pad with zeros\n","                    pad = np.zeros((self.batch_size - batch.shape[0], batch.shape[1]), dtype=batch.dtype)\n","                    batch = np.vstack((batch, pad))\n","                sess.run(tf.variables_initializer(self.random_variables))  # Need to re-sample from uniform distribution\n","                sess.run([self.update_W, self.update_b, self.update_c],\n","                         feed_dict={self.visible_units_placeholder: batch})\n","            if self.verbose:\n","                error = self._compute_reconstruction_error(data)\n","                print(\">> Epoch %d finished \\tRBM Reconstruction error %f\" % (iteration, error))\n","\n","    def _compute_hidden_units_matrix(self, matrix_visible_units):\n","        \"\"\"\n","        Computes hidden unit outputs.\n","        :param matrix_visible_units: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        return sess.run(self.compute_hidden_units_op,\n","                        feed_dict={self.visible_units_placeholder: matrix_visible_units})\n","\n","    def _compute_visible_units_matrix(self, matrix_hidden_units):\n","        \"\"\"\n","        Computes visible (or input) unit outputs.\n","        :param matrix_hidden_units: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        return sess.run(self.compute_visible_units_op,\n","                        feed_dict={self.hidden_units_placeholder: matrix_hidden_units})"],"metadata":{"id":"wd4-VtNt76Xm","executionInfo":{"status":"ok","timestamp":1655015793358,"user_tz":-330,"elapsed":610,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["class UnsupervisedDBN(BaseUnsupervisedDBN, BaseTensorFlowModel):\n","    \"\"\"\n","    This class implements a unsupervised Deep Belief Network in TensorFlow\n","    \"\"\"\n","\n","    def __init__(self, **kwargs):\n","        super(UnsupervisedDBN, self).__init__(**kwargs)\n","        self.rbm_class = BinaryRBM\n","\n","    @classmethod\n","    def _get_param_names(cls):\n","        return ['hidden_layers_structure',\n","                'activation_function',\n","                'optimization_algorithm',\n","                'learning_rate_rbm',\n","                'n_epochs_rbm',\n","                'contrastive_divergence_iter',\n","                'batch_size',\n","                'verbose']\n","\n","    @classmethod\n","    def _get_weight_variables_names(cls):\n","        return []\n","\n","    def to_dict(self):\n","        dct_to_save = super(UnsupervisedDBN, self).to_dict()\n","        dct_to_save['rbm_layers'] = [rbm.to_dict() for rbm in self.rbm_layers]\n","        return dct_to_save\n","\n","    @classmethod\n","    def from_dict(cls, dct_to_load):\n","        rbm_layers = dct_to_load.pop('rbm_layers')\n","        instance = cls(**dct_to_load)\n","        setattr(instance, 'rbm_layers', [instance.rbm_class.from_dict(rbm) for rbm in rbm_layers])\n","        return instance"],"metadata":{"id":"-7yqt8iH8Bhn","executionInfo":{"status":"ok","timestamp":1655015801546,"user_tz":-330,"elapsed":513,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["class TensorFlowAbstractSupervisedDBN(BaseAbstractSupervisedDBN, BaseTensorFlowModel):\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self, **kwargs):\n","        super(TensorFlowAbstractSupervisedDBN, self).__init__(UnsupervisedDBN, **kwargs)\n","\n","    @classmethod\n","    def _get_param_names(cls):\n","        return ['n_iter_backprop',\n","                'l2_regularization',\n","                'learning_rate',\n","                'batch_size',\n","                'dropout_p',\n","                'verbose']\n","\n","    @classmethod\n","    def _get_weight_variables_names(cls):\n","        return ['W', 'b']\n","\n","    def _initialize_weights(self, weights):\n","        if weights:\n","            for attr_name, value in weights.items():\n","                self.__setattr__(attr_name, tf.Variable(value))\n","        else:\n","            if self.unsupervised_dbn.activation_function == 'sigmoid':\n","                stddev = 1.0 / np.sqrt(self.input_units)\n","                self.W = weight_variable(tf.random_normal, [self.input_units, self.num_classes], stddev)\n","                self.b = weight_variable(tf.random_normal, [self.num_classes], stddev)\n","                self._activation_function_class = tf.nn.sigmoid\n","            elif self.unsupervised_dbn.activation_function == 'relu':\n","                stddev = 0.1 / np.sqrt(self.input_units)\n","                self.W = weight_variable(tf.truncated_normal, [self.input_units, self.num_classes], stddev)\n","                self.b = bias_variable(stddev, [self.num_classes])\n","                self._activation_function_class = tf.nn.relu\n","            else:\n","                raise ValueError(\"Invalid activation function.\")\n","\n","    def to_dict(self):\n","        dct_to_save = super(TensorFlowAbstractSupervisedDBN, self).to_dict()\n","        dct_to_save['unsupervised_dbn'] = self.unsupervised_dbn.to_dict()\n","        dct_to_save['num_classes'] = self.num_classes\n","        return dct_to_save\n","\n","    @classmethod\n","    def from_dict(cls, dct_to_load):\n","        weights = {var_name: dct_to_load.pop(var_name) for var_name in cls._get_weight_variables_names()}\n","        unsupervised_dbn_dct = dct_to_load.pop('unsupervised_dbn')\n","        num_classes = dct_to_load.pop('num_classes')\n","\n","        instance = cls(**dct_to_load)\n","\n","        setattr(instance, 'unsupervised_dbn', instance.unsupervised_dbn_class.from_dict(unsupervised_dbn_dct))\n","        setattr(instance, 'num_classes', num_classes)\n","\n","        # Initialize RBM parameters\n","        instance._build_model(weights)\n","        sess.run(tf.variables_initializer([getattr(instance, name) for name in cls._get_weight_variables_names()]))\n","        return instance\n","\n","    def _build_model(self, weights=None):\n","        self.visible_units_placeholder = self.unsupervised_dbn.rbm_layers[0].visible_units_placeholder\n","        keep_prob = tf.placeholder(tf.float32)\n","        visible_units_placeholder_drop = tf.nn.dropout(self.visible_units_placeholder, keep_prob)\n","        self.keep_prob_placeholders = [keep_prob]\n","\n","        # Define tensorflow operation for a forward pass\n","        rbm_activation = visible_units_placeholder_drop\n","        for rbm in self.unsupervised_dbn.rbm_layers:\n","            rbm_activation = rbm._activation_function_class(\n","                tf.transpose(tf.matmul(rbm.W, tf.transpose(rbm_activation))) + rbm.c)\n","            keep_prob = tf.placeholder(tf.float32)\n","            self.keep_prob_placeholders.append(keep_prob)\n","            rbm_activation = tf.nn.dropout(rbm_activation, keep_prob)\n","\n","        self.transform_op = rbm_activation\n","        self.input_units = self.unsupervised_dbn.rbm_layers[-1].n_hidden_units\n","\n","        # weights and biases\n","        self._initialize_weights(weights)\n","\n","        if self.unsupervised_dbn.optimization_algorithm == 'sgd':\n","            self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n","        else:\n","            raise ValueError(\"Invalid optimization algorithm.\")\n","\n","        # operations\n","        self.y = tf.matmul(self.transform_op, self.W) + self.b\n","        self.y_ = tf.placeholder(tf.float32, shape=[None, self.num_classes])\n","        self.train_step = None\n","        self.cost_function = None\n","        self.output = None\n","\n","    def _fine_tuning(self, data, _labels):\n","        self.num_classes = self._determine_num_output_neurons(_labels)\n","        if self.num_classes == 1:\n","            _labels = np.expand_dims(_labels, -1)\n","\n","        self._build_model()\n","        sess.run(tf.variables_initializer([self.W, self.b]))\n","\n","        labels = self._transform_labels_to_network_format(_labels)\n","\n","        if self.verbose:\n","            print(\"[START] Fine tuning step:\")\n","        self._stochastic_gradient_descent(data, labels)\n","        if self.verbose:\n","            print(\"[END] Fine tuning step\")\n","\n","    def _stochastic_gradient_descent(self, data, labels):\n","        for iteration in range(self.n_iter_backprop):\n","            for batch_data, batch_labels in batch_generator(self.batch_size, data, labels):\n","                feed_dict = {self.visible_units_placeholder: batch_data,\n","                             self.y_: batch_labels}\n","                feed_dict.update({placeholder: self.p for placeholder in self.keep_prob_placeholders})\n","                sess.run(self.train_step, feed_dict=feed_dict)\n","\n","            if self.verbose:\n","                feed_dict = {self.visible_units_placeholder: data, self.y_: labels}\n","                feed_dict.update({placeholder: 1.0 for placeholder in self.keep_prob_placeholders})\n","                error = sess.run(self.cost_function, feed_dict=feed_dict)\n","                print(\">> Epoch %d finished \\tANN training loss %f\" % (iteration, error))\n","\n","    def transform(self, X):\n","        feed_dict = {self.visible_units_placeholder: X}\n","        feed_dict.update({placeholder: 1.0 for placeholder in self.keep_prob_placeholders})\n","        return sess.run(self.transform_op,\n","                        feed_dict=feed_dict)\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predicts the target given data.\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        if len(X.shape) == 1:  # It is a single sample\n","            X = np.expand_dims(X, 0)\n","        predicted_data = self._compute_output_units_matrix(X)\n","        return predicted_data\n","\n","    def _compute_output_units_matrix(self, matrix_visible_units):\n","        feed_dict = {self.visible_units_placeholder: matrix_visible_units}\n","        feed_dict.update({placeholder: 1.0 for placeholder in self.keep_prob_placeholders})\n","        return sess.run(self.output, feed_dict=feed_dict)"],"metadata":{"id":"a2LzYpeq8Krc","executionInfo":{"status":"ok","timestamp":1655015805107,"user_tz":-330,"elapsed":469,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["class SupervisedDBNClassification(TensorFlowAbstractSupervisedDBN, ClassifierMixin):\n","    \"\"\"\n","    This class implements a Deep Belief Network for classification problems.\n","    It appends a Softmax Linear Classifier as output layer.\n","    \"\"\"\n","\n","    def _build_model(self, weights=None):\n","        super(SupervisedDBNClassification, self)._build_model(weights)\n","        self.output = tf.nn.softmax(self.y)\n","        self.cost_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.y, labels=tf.stop_gradient(self.y_)))\n","        self.train_step = self.optimizer.minimize(self.cost_function)\n","\n","    @classmethod\n","    def _get_param_names(cls):\n","        return super(SupervisedDBNClassification, cls)._get_param_names() + ['label_to_idx_map', 'idx_to_label_map']\n","\n","    @classmethod\n","    def from_dict(cls, dct_to_load):\n","        label_to_idx_map = dct_to_load.pop('label_to_idx_map')\n","        idx_to_label_map = dct_to_load.pop('idx_to_label_map')\n","\n","        instance = super(SupervisedDBNClassification, cls).from_dict(dct_to_load)\n","        setattr(instance, 'label_to_idx_map', label_to_idx_map)\n","        setattr(instance, 'idx_to_label_map', idx_to_label_map)\n","\n","        return instance\n","\n","    def _transform_labels_to_network_format(self, labels):\n","        new_labels, label_to_idx_map, idx_to_label_map = to_categorical(labels, self.num_classes)\n","        self.label_to_idx_map = label_to_idx_map\n","        self.idx_to_label_map = idx_to_label_map\n","        return new_labels\n","\n","    def _transform_network_format_to_labels(self, indexes):\n","        \"\"\"\n","        Converts network output to original labels.\n","        :param indexes: array-like, shape = (n_samples, )\n","        :return:\n","        \"\"\"\n","        return list(map(lambda idx: self.idx_to_label_map[idx], indexes))\n","\n","    def predict(self, X):\n","        probs = self.predict_proba(X)\n","        indexes = np.argmax(probs, axis=1)\n","        return self._transform_network_format_to_labels(indexes)\n","\n","    def predict_proba(self, X):\n","        \"\"\"\n","        Predicts probability distribution of classes for each sample in the given data.\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        return super(SupervisedDBNClassification, self)._compute_output_units_matrix(X)\n","\n","    def predict_proba_dict(self, X):\n","        \"\"\"\n","        Predicts probability distribution of classes for each sample in the given data.\n","        Returns a list of dictionaries, one per sample. Each dict contains {label_1: prob_1, ..., label_j: prob_j}\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        if len(X.shape) == 1:  # It is a single sample\n","            X = np.expand_dims(X, 0)\n","\n","        predicted_probs = self.predict_proba(X)\n","\n","        result = []\n","        num_of_data, num_of_labels = predicted_probs.shape\n","        for i in range(num_of_data):\n","            # key : label\n","            # value : predicted probability\n","            dict_prob = {}\n","            for j in range(num_of_labels):\n","                dict_prob[self.idx_to_label_map[j]] = predicted_probs[i][j]\n","            result.append(dict_prob)\n","\n","        return result\n","\n","    def _determine_num_output_neurons(self, labels):\n","        return len(np.unique(labels))\n"],"metadata":{"id":"uIDo1nlt8WYY","executionInfo":{"status":"ok","timestamp":1655015811562,"user_tz":-330,"elapsed":390,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["class SupervisedDBNRegression(TensorFlowAbstractSupervisedDBN, RegressorMixin):\n","    \"\"\"\n","    This class implements a Deep Belief Network for regression problems in TensorFlow.\n","    \"\"\"\n","\n","    def _build_model(self, weights=None):\n","        super(SupervisedDBNRegression, self)._build_model(weights)\n","        self.output = self.y\n","        self.cost_function = tf.reduce_mean(tf.square(self.y_ - self.y))  # Mean Squared Error\n","        self.train_step = self.optimizer.minimize(self.cost_function)\n","\n","    def _transform_labels_to_network_format(self, labels):\n","        \"\"\"\n","        Returns the same labels since regression case does not need to convert anything.\n","        :param labels: array-like, shape = (n_samples, targets)\n","        :return:\n","        \"\"\"\n","        return labels\n","\n","    def _compute_output_units_matrix(self, matrix_visible_units):\n","        return super(SupervisedDBNRegression, self)._compute_output_units_matrix(matrix_visible_units)\n","\n","    def _determine_num_output_neurons(self, labels):\n","        if len(labels.shape) == 1:\n","            return 1\n","        else:\n","            return labels.shape[1]"],"metadata":{"id":"y6UOeEVR9RpM","executionInfo":{"status":"ok","timestamp":1655015817237,"user_tz":-330,"elapsed":400,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["# **Classification Of Fingerprint Images**"],"metadata":{"id":"2h6daGDAS8gW"}},{"cell_type":"markdown","source":["> **NOTE:** classification is performed on fingerprint images using **deep belief network**. There are total two categories **Real** and **Fake**."],"metadata":{"id":"x_FIScvyD7kj"}},{"cell_type":"markdown","source":["## **0. Libraries**"],"metadata":{"id":"9l_xxCShFbOC"}},{"cell_type":"code","source":["# Libraries for phase 1 and 2\n","import cv2\n","import numpy as np\n","import os.path\n","\n","# Libraries for phase 3\n","from sklearn.preprocessing import StandardScaler\n","\n","# Libraries for phase 4\n","from sklearn.decomposition import PCA\n","from sklearn.utils import shuffle\n","\n","# Libraries for phase 5\n","from sklearn.model_selection import train_test_split\n","\n","# Libraries for phase 7\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"jKK462cTS_5d","executionInfo":{"status":"ok","timestamp":1655015821490,"user_tz":-330,"elapsed":899,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["## **1. Loading Data**"],"metadata":{"id":"waz4djTw73zJ"}},{"cell_type":"markdown","source":["### **Train and Test: Live**"],"metadata":{"id":"dHAsr1tzGmWE"}},{"cell_type":"code","source":["\n","\"\"\"\n","      1.  Loading live category images from training and testing directories.\n","      2.  Converting every images into one dimensional array. Because DBN (Deep\n","          Belief Network) takes input in only single dimensional array of an image.\n","\"\"\"\n","\n","live_image_array = [] \n","\n","\"\"\" \n","      Empty list to store live images of training and testing \n","      directories and then convert list into numpy array \n","\"\"\"\n","\n","train_live_path = '/content/drive/MyDrive/BISAG Internship/Dataset/GitHub_Dataset/training/Live/'\n","test_live_path = '/content/drive/MyDrive/BISAG Internship/Dataset/GitHub_Dataset/testing/Live/'\n","live_paths = [train_live_path,test_live_path]\n","\n","\n","for iterVar_Zero in range(1,3): # Two Directories\n","  for iterVar_One in range(1,41): # Total 40 major images\n","    for iterVar_Two in range(1,6): # Each major image contains 5 minor images\n","\n","      image_name = str(iterVar_One) + '_' + str(iterVar_Two) + '.png'\n","\n","      # to change directory path for next iteration of loop one \n","      if iterVar_Zero == 1:\n","        image_path = live_paths[0] + image_name\n","      else:\n","        image_path = live_paths[1] + image_name\n","      \n","      input_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE) # reading image into grayscale\n","      resized_image = cv2.resize(input_image, (50,50)) # resizing image with (50,50) dimension tupple\n","      resized_image_array = np.asarray(resized_image) # converting image into numpy array\n","      flatten_array = resized_image_array.flatten() # converting 2D image_array into 1D image_array\n","      live_image_array.append(flatten_array) # appending to list\n","\n","X_live = np.array(live_image_array) # generating X_live set from training-testing category"],"metadata":{"id":"FO6_hUPqHmb0","executionInfo":{"status":"ok","timestamp":1655015933708,"user_tz":-330,"elapsed":109914,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["X_live.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"wV5pkm98K5S7","executionInfo":{"status":"ok","timestamp":1655015937950,"user_tz":-330,"elapsed":401,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}},"outputId":"f4c1a20e-e28c-425e-931c-5a73a0931c0c"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(400, 2500)"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["\"\"\" Empty list to store category label \"\"\"\n","live_category = []\n","\n","\"\"\" Total 400 live images = 200 training set + 200 testing set\"\"\"\n","for iterVar_Three in range(1,401): \n","  live_category.append(1)\n","\n","Y_live = np.array(live_category) # converting list to numpy array and generating Y_live"],"metadata":{"id":"PhnTUqB-LtUN","executionInfo":{"status":"ok","timestamp":1655015943294,"user_tz":-330,"elapsed":629,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["Y_live.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"qYDHbI7jAbvv","executionInfo":{"status":"ok","timestamp":1655015945222,"user_tz":-330,"elapsed":7,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}},"outputId":"d601ded3-abfd-4137-986a-7b65eb09cb4f"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(400,)"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["### **Train and Test: Fake**"],"metadata":{"id":"WCXfD_bcGvFV"}},{"cell_type":"code","source":["\n","\"\"\"\n","      1.  Loading fake category images from training and testing directories.\n","      2.  Converting every images into one dimensional array. Because DBN (Deep\n","          Belief Network) takes input in only single dimensional array of an image.\n","\"\"\"\n","\n","fake_image_array = [] \n","\"\"\" \n","      Empty list to store live images of training and testing\n","      directories and then convert list into numpy array \n","\"\"\"\n","\n","train_fake_path = '/content/drive/MyDrive/BISAG Internship/Dataset/GitHub_Dataset/training/Fake/'\n","test_fake_path = '/content/drive/MyDrive/BISAG Internship/Dataset/GitHub_Dataset/testing/Fake/'\n","fake_paths = [train_fake_path,test_fake_path]\n","\n","\n","for iterVar_Four in range(1,3): # Two Directories\n","  for iterVar_Five in range(1,200): # Uknown number if images\n","    for iterVar_Six in range(1,11): # Each major image contains 10 minor images\n","\n","      image_name = str(iterVar_Five) + '_' + str(iterVar_Six) + '.png'\n","\n","      # to change directory path for next iteration of loop one \n","      if iterVar_Four == 1:\n","        image_path = fake_paths[0] + image_name\n","      else:\n","        image_path = fake_paths[1] + image_name\n","      \n","      \"\"\" for fake images there is no continuation in image name like live \n","      images so it is required to check that image_name generated based on loop\n","      iterations is valid or not \"\"\"\n","\n","      if os.path.isfile(image_path):\n","        input_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE) # reading image into grayscale\n","        resized_image = cv2.resize(input_image, (50,50)) # resizing image with (50,50) dimension tupple\n","        resized_image_array = np.asarray(resized_image) # converting image into numpy array\n","        flatten_array = resized_image_array.flatten() # converting 2D image_array into 1D image_array\n","        fake_image_array.append(flatten_array) # appending to list\n","\n","X_fake = np.array(fake_image_array) # generating X_fake set from training-testing category"],"metadata":{"id":"OKhwdp9WQX0F","executionInfo":{"status":"ok","timestamp":1655016048760,"user_tz":-330,"elapsed":101786,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["X_fake.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"h2FPEF0pUq_1","executionInfo":{"status":"ok","timestamp":1655016053486,"user_tz":-330,"elapsed":705,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}},"outputId":"0af7cb2d-6807-4f90-ed3a-8a8676e4baa6"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(400, 2500)"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["\"\"\" Empty list to store category label \"\"\"\n","fake_category = [] \n","\n","\"\"\" do not know the actual number of fake images\n","    in training-testing directory that is why using \n","    dimension of X_fake iterations limit.\n","\"\"\"\n","for iterVar_Seven in range(X_fake.shape[0]): \n","  fake_category.append(0)\n","\n","Y_fake = np.array(fake_category) # converting list to numpy array and generating Y_fake"],"metadata":{"id":"f5TvmaSQTHqv","executionInfo":{"status":"ok","timestamp":1655016055013,"user_tz":-330,"elapsed":2,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["## **2. Merging**"],"metadata":{"id":"KA88y2OQUKqR"}},{"cell_type":"markdown","source":["> combining **X_live** and **X_fake** ---- **Y_live** and **Y_fake**"],"metadata":{"id":"yIhtmn9RUYGR"}},{"cell_type":"code","source":["\"\"\"\n","      Combining X_live and X_fake to generate intermediate dataset: X_live_fake\n","\"\"\"\n","\n","X_live_fake = list(X_live) # converting X_live into list and then assigning to X_live_fake\n","X_fake = list(X_fake)\n","\n","for iterVar_Eight in range(len(X_fake)):\n","  X_live_fake.append(X_fake[iterVar_Eight])\n","\n","X_live_fake = np.asarray(X_live_fake) # generating X_live_fake \n","X_live_fake.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"V3eGrAbqUl1v","executionInfo":{"status":"ok","timestamp":1655016057653,"user_tz":-330,"elapsed":3,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}},"outputId":"fc5040cf-8c78-434a-fcea-78a6535c84f9"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(800, 2500)"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["\"\"\"\n","      Combining Y_live and Y_fake to generate intermediate dataset: Y_live_fake\n","\"\"\"\n","\n","Y_live_fake = list(Y_live) # converting Y_live into list and then assigning to Y_live_fake\n","Y_fake = list(Y_fake)\n","\n","for iterVar_Nine in range(len(Y_fake)):\n","  Y_live_fake.append(Y_fake[iterVar_Nine])\n","\n","Y_live_fake = np.asarray(Y_live_fake) # generating Y_live_fake\n","Y_live_fake.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"cAKnxM8gWH3w","executionInfo":{"status":"ok","timestamp":1655016060357,"user_tz":-330,"elapsed":5,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}},"outputId":"b9574a77-aab6-485e-a80f-3b402537b7a4"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(800,)"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["## **3. Scaling 1D Image Array**"],"metadata":{"id":"axUxipFCIzVD"}},{"cell_type":"code","source":["\"\"\" applying standard scaler \"\"\"\n","\n","standard_scaler = StandardScaler() # creating object of class StandardScaler\n","\n","\"\"\" scaled dataset of arrays of images to increase model performance \"\"\"\n","\n","X_scaled = standard_scaler.fit_transform(X_live_fake) "],"metadata":{"id":"G1RsK7ey5tWR","executionInfo":{"status":"ok","timestamp":1655016065378,"user_tz":-330,"elapsed":4,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["## **4. Principal Component Analysis**"],"metadata":{"id":"xIIHfAb-Zn4p"}},{"cell_type":"markdown","source":["> Applying PCA for **dimensionality reduction**"],"metadata":{"id":"t4hRX-PwZrzO"}},{"cell_type":"code","source":["\"\"\" reducing features of single dimensional array from too many to 64 \"\"\"\n","\n","pca = PCA(n_components=64)\n","X_reduced = pca.fit_transform(X_scaled) # fit and trasforming data using object of PCA\n","\n","X_reduced, Y_live_fake = shuffle(X_reduced, Y_live_fake) # shuffle dataset for good model learning \n"],"metadata":{"id":"2ANU330WZnLG","executionInfo":{"status":"ok","timestamp":1655016071098,"user_tz":-330,"elapsed":1085,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["## **5. Dataset Splitting**"],"metadata":{"id":"sNj5i3vd6ZH_"}},{"cell_type":"code","source":["\"\"\" \n","      training-testing ratio: \n","      75%-25% \n","\"\"\"\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X_reduced, Y_live_fake, test_size=0.25, random_state=0)"],"metadata":{"id":"XTqshdtlTVP_","executionInfo":{"status":"ok","timestamp":1655016082648,"user_tz":-330,"elapsed":536,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["## **6. DBN Classifier**"],"metadata":{"id":"2T6OwcSy72-0"}},{"cell_type":"code","source":["\"\"\" relu is also available as activation_function.\n","    sigmoid is performing better than relu activation_function \"\"\"\n","\n","DBNClassifier = SupervisedDBNClassification(hidden_layers_structure=[256,256], \n","                                            learning_rate_rbm = 0.1,\n","                                            learning_rate = 0.1,\n","                                            n_epochs_rbm = 40,\n","                                            n_iter_backprop = 20,\n","                                            batch_size = 32,\n","                                            activation_function = 'sigmoid',\n","                                            dropout_p = 0.2)"],"metadata":{"id":"NykDj_Q_Yr51","executionInfo":{"status":"ok","timestamp":1655016086214,"user_tz":-330,"elapsed":618,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["DBNClassifier.fit(X_train, Y_train) # fitting data to the model"],"metadata":{"id":"DV_hP_XC54vf","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1655016147899,"user_tz":-330,"elapsed":59947,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}},"outputId":"f0a5db45-eb74-4612-cabc-bac5da42bd10"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["[START] Pre-training step:\n","WARNING:tensorflow:From <ipython-input-20-222d3512e59e>:78: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n",">> Epoch 1 finished \tRBM Reconstruction error 1457.030454\n",">> Epoch 2 finished \tRBM Reconstruction error 1458.225190\n",">> Epoch 3 finished \tRBM Reconstruction error 1457.489310\n",">> Epoch 4 finished \tRBM Reconstruction error 1457.687460\n",">> Epoch 5 finished \tRBM Reconstruction error 1458.719450\n",">> Epoch 6 finished \tRBM Reconstruction error 1458.806159\n",">> Epoch 7 finished \tRBM Reconstruction error 1459.865522\n",">> Epoch 8 finished \tRBM Reconstruction error 1459.573541\n",">> Epoch 9 finished \tRBM Reconstruction error 1459.763385\n",">> Epoch 10 finished \tRBM Reconstruction error 1460.520554\n",">> Epoch 11 finished \tRBM Reconstruction error 1459.374801\n",">> Epoch 12 finished \tRBM Reconstruction error 1460.580637\n",">> Epoch 13 finished \tRBM Reconstruction error 1459.388765\n",">> Epoch 14 finished \tRBM Reconstruction error 1460.073316\n",">> Epoch 15 finished \tRBM Reconstruction error 1459.696666\n",">> Epoch 16 finished \tRBM Reconstruction error 1459.504348\n",">> Epoch 17 finished \tRBM Reconstruction error 1460.285373\n",">> Epoch 18 finished \tRBM Reconstruction error 1460.093827\n",">> Epoch 19 finished \tRBM Reconstruction error 1459.859531\n",">> Epoch 20 finished \tRBM Reconstruction error 1459.393817\n",">> Epoch 21 finished \tRBM Reconstruction error 1460.151949\n",">> Epoch 22 finished \tRBM Reconstruction error 1460.478462\n",">> Epoch 23 finished \tRBM Reconstruction error 1460.076590\n",">> Epoch 24 finished \tRBM Reconstruction error 1460.657849\n",">> Epoch 25 finished \tRBM Reconstruction error 1460.131147\n",">> Epoch 26 finished \tRBM Reconstruction error 1459.954208\n",">> Epoch 27 finished \tRBM Reconstruction error 1460.209549\n",">> Epoch 28 finished \tRBM Reconstruction error 1459.947379\n",">> Epoch 29 finished \tRBM Reconstruction error 1459.201846\n",">> Epoch 30 finished \tRBM Reconstruction error 1460.032882\n",">> Epoch 31 finished \tRBM Reconstruction error 1459.970098\n",">> Epoch 32 finished \tRBM Reconstruction error 1460.148515\n",">> Epoch 33 finished \tRBM Reconstruction error 1460.621394\n",">> Epoch 34 finished \tRBM Reconstruction error 1460.200822\n",">> Epoch 35 finished \tRBM Reconstruction error 1459.695293\n",">> Epoch 36 finished \tRBM Reconstruction error 1460.483402\n",">> Epoch 37 finished \tRBM Reconstruction error 1459.997548\n",">> Epoch 38 finished \tRBM Reconstruction error 1460.505172\n",">> Epoch 39 finished \tRBM Reconstruction error 1458.736137\n",">> Epoch 40 finished \tRBM Reconstruction error 1459.037794\n",">> Epoch 1 finished \tRBM Reconstruction error 7.177505\n",">> Epoch 2 finished \tRBM Reconstruction error 5.091179\n",">> Epoch 3 finished \tRBM Reconstruction error 4.818528\n",">> Epoch 4 finished \tRBM Reconstruction error 4.422761\n",">> Epoch 5 finished \tRBM Reconstruction error 4.134650\n",">> Epoch 6 finished \tRBM Reconstruction error 3.992262\n",">> Epoch 7 finished \tRBM Reconstruction error 3.780888\n",">> Epoch 8 finished \tRBM Reconstruction error 3.701792\n",">> Epoch 9 finished \tRBM Reconstruction error 3.416491\n",">> Epoch 10 finished \tRBM Reconstruction error 3.287939\n",">> Epoch 11 finished \tRBM Reconstruction error 3.295260\n",">> Epoch 12 finished \tRBM Reconstruction error 3.041785\n",">> Epoch 13 finished \tRBM Reconstruction error 3.118111\n",">> Epoch 14 finished \tRBM Reconstruction error 2.968637\n",">> Epoch 15 finished \tRBM Reconstruction error 2.897289\n",">> Epoch 16 finished \tRBM Reconstruction error 2.803290\n",">> Epoch 17 finished \tRBM Reconstruction error 2.826109\n",">> Epoch 18 finished \tRBM Reconstruction error 2.784442\n",">> Epoch 19 finished \tRBM Reconstruction error 2.676781\n",">> Epoch 20 finished \tRBM Reconstruction error 2.756658\n",">> Epoch 21 finished \tRBM Reconstruction error 2.641187\n",">> Epoch 22 finished \tRBM Reconstruction error 2.545204\n",">> Epoch 23 finished \tRBM Reconstruction error 2.555558\n",">> Epoch 24 finished \tRBM Reconstruction error 2.468622\n",">> Epoch 25 finished \tRBM Reconstruction error 2.383386\n",">> Epoch 26 finished \tRBM Reconstruction error 2.407108\n",">> Epoch 27 finished \tRBM Reconstruction error 2.393528\n",">> Epoch 28 finished \tRBM Reconstruction error 2.288145\n",">> Epoch 29 finished \tRBM Reconstruction error 2.298423\n",">> Epoch 30 finished \tRBM Reconstruction error 2.181411\n",">> Epoch 31 finished \tRBM Reconstruction error 2.275267\n",">> Epoch 32 finished \tRBM Reconstruction error 2.142282\n",">> Epoch 33 finished \tRBM Reconstruction error 2.145862\n",">> Epoch 34 finished \tRBM Reconstruction error 2.155184\n",">> Epoch 35 finished \tRBM Reconstruction error 2.078407\n",">> Epoch 36 finished \tRBM Reconstruction error 2.051395\n",">> Epoch 37 finished \tRBM Reconstruction error 2.037449\n",">> Epoch 38 finished \tRBM Reconstruction error 1.981997\n",">> Epoch 39 finished \tRBM Reconstruction error 2.062404\n",">> Epoch 40 finished \tRBM Reconstruction error 1.936347\n","[END] Pre-training step\n","WARNING:tensorflow:From <ipython-input-22-d341e4eae5cd>:63: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","[START] Fine tuning step:\n",">> Epoch 0 finished \tANN training loss 0.524047\n",">> Epoch 1 finished \tANN training loss 0.528016\n",">> Epoch 2 finished \tANN training loss 0.539881\n",">> Epoch 3 finished \tANN training loss 0.528891\n",">> Epoch 4 finished \tANN training loss 0.540037\n",">> Epoch 5 finished \tANN training loss 0.565654\n",">> Epoch 6 finished \tANN training loss 0.541389\n",">> Epoch 7 finished \tANN training loss 0.693894\n",">> Epoch 8 finished \tANN training loss 0.549272\n",">> Epoch 9 finished \tANN training loss 0.630911\n",">> Epoch 10 finished \tANN training loss 0.477954\n",">> Epoch 11 finished \tANN training loss 0.533249\n",">> Epoch 12 finished \tANN training loss 0.732447\n",">> Epoch 13 finished \tANN training loss 0.858123\n",">> Epoch 14 finished \tANN training loss 0.514771\n",">> Epoch 15 finished \tANN training loss 0.573784\n",">> Epoch 16 finished \tANN training loss 0.481439\n",">> Epoch 17 finished \tANN training loss 0.476490\n",">> Epoch 18 finished \tANN training loss 0.480273\n",">> Epoch 19 finished \tANN training loss 0.518660\n","[END] Fine tuning step\n"]},{"output_type":"execute_result","data":{"text/plain":["SupervisedDBNClassification(batch_size=32, dropout_p=0.2,\n","                            idx_to_label_map={0: 1, 1: 0},\n","                            l2_regularization=1.0,\n","                            label_to_idx_map={0: 1, 1: 0}, learning_rate=0.1,\n","                            n_iter_backprop=20, verbose=True)"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["## **7. Testing Phase**"],"metadata":{"id":"DO8VSvKM8tfo"}},{"cell_type":"code","source":["\"\"\"\" performance measuring based on accuracy \"\"\"\n","\n","Y_pred = DBNClassifier.predict(X_test)\n","print('Done.\\nAccuracy: %f' % accuracy_score(Y_test, Y_pred)) # testing accuracy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"1AmzPDMGTYEZ","executionInfo":{"status":"ok","timestamp":1655016163425,"user_tz":-330,"elapsed":518,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}},"outputId":"bc39269a-7bdf-466d-f28b-ae1c037baea6"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Done.\n","Accuracy: 0.780000\n"]}]},{"cell_type":"code","source":["\"\"\"\" showing probability for each category with an integer label \"\"\"\n","\n","Y_pred = DBNClassifier.predict_proba_dict(X_test)\n","Y_pred"],"metadata":{"id":"SdhL2Ig9NZA_","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1655016178170,"user_tz":-330,"elapsed":514,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}},"outputId":"5ef6a975-71a7-4584-9f33-ec7760f29580"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{0: 0.6340841, 1: 0.36591586},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.96863425, 1: 0.031365767},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.9458624, 1: 0.05413757},\n"," {0: 0.9154792, 1: 0.08452087},\n"," {0: 0.931674, 1: 0.06832607},\n"," {0: 0.45902964, 1: 0.5409704},\n"," {0: 0.93393135, 1: 0.0660686},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.9160603, 1: 0.08393963},\n"," {0: 0.17135197, 1: 0.82864803},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.42957345, 1: 0.5704266},\n"," {0: 0.26785383, 1: 0.73214626},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.4878377, 1: 0.5121623},\n"," {0: 0.93414736, 1: 0.065852664},\n"," {0: 0.403244, 1: 0.596756},\n"," {0: 0.4043923, 1: 0.5956077},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.96215755, 1: 0.037842438},\n"," {0: 0.6340841, 1: 0.36591586},\n"," {0: 0.87955296, 1: 0.12044698},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.9263655, 1: 0.07363453},\n"," {0: 0.95719105, 1: 0.042808924},\n"," {0: 0.6340841, 1: 0.36591586},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.18105938, 1: 0.81894064},\n"," {0: 0.3847185, 1: 0.61528146},\n"," {0: 0.25490624, 1: 0.74509376},\n"," {0: 0.45902964, 1: 0.5409704},\n"," {0: 0.9723923, 1: 0.027607702},\n"," {0: 0.9393168, 1: 0.060683195},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.3083933, 1: 0.6916067},\n"," {0: 0.89188594, 1: 0.10811406},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.30407432, 1: 0.69592565},\n"," {0: 0.38603258, 1: 0.61396736},\n"," {0: 0.6340841, 1: 0.36591586},\n"," {0: 0.9698691, 1: 0.030130967},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.24511868, 1: 0.7548813},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.6340841, 1: 0.36591586},\n"," {0: 0.96531236, 1: 0.034687586},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.39858732, 1: 0.60141265},\n"," {0: 0.45902964, 1: 0.5409704},\n"," {0: 0.9123938, 1: 0.08760618},\n"," {0: 0.89188594, 1: 0.10811406},\n"," {0: 0.45902964, 1: 0.5409704},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.97846895, 1: 0.021530997},\n"," {0: 0.9118646, 1: 0.0881354},\n"," {0: 0.9703907, 1: 0.029609295},\n"," {0: 0.9130697, 1: 0.08693024},\n"," {0: 0.30055234, 1: 0.69944763},\n"," {0: 0.9611542, 1: 0.03884577},\n"," {0: 0.7282959, 1: 0.27170405},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.93472207, 1: 0.06527794},\n"," {0: 0.45902964, 1: 0.5409704},\n"," {0: 0.9350026, 1: 0.06499731},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.30173576, 1: 0.69826424},\n"," {0: 0.6340841, 1: 0.36591586},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.6749802, 1: 0.32501984},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.27959156, 1: 0.72040844},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.30088985, 1: 0.6991102},\n"," {0: 0.9346408, 1: 0.06535922},\n"," {0: 0.45902964, 1: 0.5409704},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.40377718, 1: 0.5962228},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.6340841, 1: 0.36591586},\n"," {0: 0.8596558, 1: 0.14034419},\n"," {0: 0.89188594, 1: 0.10811406},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.45902964, 1: 0.5409704},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.40303215, 1: 0.5969678},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.9208542, 1: 0.07914576},\n"," {0: 0.6340841, 1: 0.36591586},\n"," {0: 0.3362553, 1: 0.6637446},\n"," {0: 0.28289825, 1: 0.7171018},\n"," {0: 0.6340841, 1: 0.36591586},\n"," {0: 0.9454113, 1: 0.05458872},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.40686515, 1: 0.59313476},\n"," {0: 0.84418243, 1: 0.15581754},\n"," {0: 0.90476274, 1: 0.0952373},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.25248098, 1: 0.7475191},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.2712666, 1: 0.72873336},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.91120124, 1: 0.08879878},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.9710669, 1: 0.028933141},\n"," {0: 0.9694958, 1: 0.03050419},\n"," {0: 0.6749802, 1: 0.32501984},\n"," {0: 0.6340841, 1: 0.36591586},\n"," {0: 0.9025917, 1: 0.09740827},\n"," {0: 0.96635604, 1: 0.03364394},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.45902964, 1: 0.5409704},\n"," {0: 0.63407874, 1: 0.36592126},\n"," {0: 0.96222246, 1: 0.03777762},\n"," {0: 0.9763417, 1: 0.02365832},\n"," {0: 0.27886748, 1: 0.7211325},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.42957345, 1: 0.5704266},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.1727059, 1: 0.82729405},\n"," {0: 0.37928036, 1: 0.6207197},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.2512293, 1: 0.74877065},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.1973803, 1: 0.8026197},\n"," {0: 0.25940335, 1: 0.74059665},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.8241745, 1: 0.17582548},\n"," {0: 0.9812525, 1: 0.018747509},\n"," {0: 0.39541283, 1: 0.60458714},\n"," {0: 0.45902964, 1: 0.5409704},\n"," {0: 0.9714225, 1: 0.02857749},\n"," {0: 0.38011825, 1: 0.61988175},\n"," {0: 0.96922743, 1: 0.030772623},\n"," {0: 0.30028078, 1: 0.6997192},\n"," {0: 0.8242611, 1: 0.17573884},\n"," {0: 0.96501887, 1: 0.034981072},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.24665469, 1: 0.75334525},\n"," {0: 0.38011736, 1: 0.61988264},\n"," {0: 0.9735127, 1: 0.026487352},\n"," {0: 0.2839678, 1: 0.7160322},\n"," {0: 0.28794843, 1: 0.7120516},\n"," {0: 0.962206, 1: 0.037794},\n"," {0: 0.3102392, 1: 0.6897608},\n"," {0: 0.45902964, 1: 0.5409704},\n"," {0: 0.2671221, 1: 0.73287785},\n"," {0: 0.37918597, 1: 0.620814},\n"," {0: 0.93514115, 1: 0.06485882},\n"," {0: 0.1368803, 1: 0.8631197},\n"," {0: 0.9094327, 1: 0.09056732},\n"," {0: 0.4062456, 1: 0.59375435},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.54993653, 1: 0.4500634},\n"," {0: 0.64361465, 1: 0.35638538},\n"," {0: 0.4030116, 1: 0.5969883},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.79639155, 1: 0.20360842},\n"," {0: 0.45902964, 1: 0.5409704},\n"," {0: 0.45902964, 1: 0.5409704},\n"," {0: 0.91368157, 1: 0.08631844},\n"," {0: 0.6340841, 1: 0.36591586},\n"," {0: 0.2638555, 1: 0.7361445},\n"," {0: 0.6340841, 1: 0.36591586},\n"," {0: 0.2700072, 1: 0.7299928},\n"," {0: 0.9349922, 1: 0.06500774},\n"," {0: 0.914074, 1: 0.08592595},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.45902964, 1: 0.5409704},\n"," {0: 0.45902964, 1: 0.5409704},\n"," {0: 0.4827828, 1: 0.5172171},\n"," {0: 0.37110287, 1: 0.62889713},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.45902964, 1: 0.5409704},\n"," {0: 0.27274087, 1: 0.7272591},\n"," {0: 0.6085135, 1: 0.3914865},\n"," {0: 0.63408417, 1: 0.36591586},\n"," {0: 0.28869444, 1: 0.7113055},\n"," {0: 0.45902967, 1: 0.5409704},\n"," {0: 0.26335067, 1: 0.7366494},\n"," {0: 0.60851353, 1: 0.39148647},\n"," {0: 0.1728968, 1: 0.8271032},\n"," {0: 0.9639848, 1: 0.036015194},\n"," {0: 0.48278284, 1: 0.51721716}]"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","source":["## **9. Saving Model**"],"metadata":{"id":"Ij_g-RZqcqFh"}},{"cell_type":"code","source":["\"\"\" \n","      For pre processing of audio and text data using pytorch \n","      install pytorch version 1.11.0 + cu113. PyTorch verison \n","      1.2.0 + cu92 is incompatible\n","\"\"\"\n","\n","!pip3 install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"M22Vuam3cskL","executionInfo":{"status":"ok","timestamp":1655016389981,"user_tz":-330,"elapsed":112702,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}},"outputId":"2b6ca20c-80b5-4f72-b203-1d71c567fe45"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.2.0+cu92\n","  Downloading https://download.pytorch.org/whl/cu92/torch-1.2.0%2Bcu92-cp37-cp37m-manylinux1_x86_64.whl (663.1 MB)\n","\u001b[K     |████████████████████████████████| 663.1 MB 1.4 kB/s \n","\u001b[?25hCollecting torchvision==0.4.0+cu92\n","  Downloading https://download.pytorch.org/whl/cu92/torchvision-0.4.0%2Bcu92-cp37-cp37m-manylinux1_x86_64.whl (8.8 MB)\n","\u001b[K     |████████████████████████████████| 8.8 MB 34.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.2.0+cu92) (1.21.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.0+cu92) (1.15.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.0+cu92) (7.1.2)\n","Installing collected packages: torch, torchvision\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.11.0+cu113\n","    Uninstalling torch-1.11.0+cu113:\n","      Successfully uninstalled torch-1.11.0+cu113\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.12.0+cu113\n","    Uninstalling torchvision-0.12.0+cu113:\n","      Successfully uninstalled torchvision-0.12.0+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.2.0+cu92 which is incompatible.\n","torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.2.0+cu92 which is incompatible.\u001b[0m\n","Successfully installed torch-1.2.0+cu92 torchvision-0.4.0+cu92\n"]}]},{"cell_type":"code","source":["\"\"\" importing torch and save module from torch \"\"\"\n","\n","import torch\n","from torch import save"],"metadata":{"id":"sHqfgBHWfGy1","executionInfo":{"status":"ok","timestamp":1655016396180,"user_tz":-330,"elapsed":415,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["model_name = 'DBN - Binary Classifier'\n","path = '/content/drive/MyDrive/BISAG Internship/Models/{model_name}'\n","torch.save(DBNClassifier.state_dict(), path)"],"metadata":{"id":"ObOV8vlUfwnq","executionInfo":{"status":"ok","timestamp":1655016840667,"user_tz":-330,"elapsed":386,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["## **8. Single Sample Testing**"],"metadata":{"id":"BoB3pNrDnz6m"}},{"cell_type":"code","source":["\"\"\"\n","      Upload single test image and applying all operations \n","      which were applied to images at training time.\n","\"\"\"\n","\n","test_image_path = '/content/drive/MyDrive/BISAG Internship/Dataset/GitHub_Dataset/testing/Live/10_4.png'\n","test_image = cv2.imread(test_image_path, cv2.IMREAD_GRAYSCALE) # reading image into grayscale\n","resized_image = cv2.resize(test_image, (50,50)) # resizing image in (50,50) dimension\n","resized_array = np.asarray(resized_image) # converting image into 2D array\n","flatten_array = resized_array.flatten() # converting 2D array into 1D array\n","test_image = flatten_array # final input image"],"metadata":{"id":"2TCgEVrUUeZv","executionInfo":{"status":"ok","timestamp":1655016853076,"user_tz":-330,"elapsed":1330,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["test_image"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"Iv9CmL-zq1rN","executionInfo":{"status":"ok","timestamp":1655016857112,"user_tz":-330,"elapsed":686,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}},"outputId":"fc51114d-e2ab-45b5-991b-fa59583b6839"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 72,  76,  77, ..., 105,  97, 100], dtype=uint8)"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["\"\"\"\n","      when standard scaler is applied to 1D test_image array,\n","      it generates zero array. do not know, why?\n","\"\"\"\n","\n","standard_scaler = StandardScaler() # creating object of StandardScaler() class\n","X_scaled = standard_scaler.fit_transform(test_image) # applying fitting and transforming process \n","\n","\"\"\"\n","      Here fit() method and transform() method are also applicable\n","      in step wise. Like first apply fitting using fit() and after transforming \n","      using transform()\n","\"\"\""],"metadata":{"id":"xZKOE9nWVoF2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","      standard scaler is not working so applying normalization\n","      using numpy function: norm()\n","\"\"\"\n","\n","norm = np.linalg.norm(test_image) # finding norm of 1D array: test_image \n","X_normalized = test_image/norm # dividing every elements using norm\n","print('Norm =========> ', norm)\n","print('Normalized Image =========> ', X_normalized)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"WAxBNKvGjOy_","executionInfo":{"status":"ok","timestamp":1655016862811,"user_tz":-330,"elapsed":7,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}},"outputId":"6c27a414-e115-4b0e-8936-da153b472ce3"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Norm =========>  7787.001797868034\n","Normalized Image =========>  [0.00924618 0.00975985 0.00988827 ... 0.01348401 0.01245666 0.01284191]\n"]}]},{"cell_type":"markdown","source":["### **PCA: Own Defined Method**"],"metadata":{"id":"PvaNfq-pkfTp"}},{"cell_type":"code","source":["\"\"\"\n","      Here input array contains only one 1D array of single test image\n","      so PCA is not applicable from sklearn library. So, still trying to\n","      apply PCA using own defined function.\n","\"\"\"\n","def PCA(X , num_components):\n","     \n","    # Step-1\n","    X_meaned = X - np.mean(X , axis = 0)\n","     \n","    # Step-2\n","    cov_mat = np.cov(X_meaned , rowvar = False)\n","     \n","    # Step-3\n","    eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)\n","     \n","    # Step-4\n","    sorted_index = np.argsort(eigen_values)[::-1]\n","    sorted_eigenvalue = eigen_values[sorted_index]\n","    sorted_eigenvectors = eigen_vectors[:,sorted_index]\n","     \n","    # Step-5\n","    eigenvector_subset = sorted_eigenvectors[:,0:num_components]\n","     \n","    # Step-6\n","    X_reduced = np.dot(eigenvector_subset.transpose() , X_meaned.transpose() ).transpose()\n","     \n","    return X_reduced\n","\n","\n","\n","X_reducedTest = PCA(X_normalized, 64) # calling function and n_components = 64"],"metadata":{"id":"dATURutRmUgy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **PCA: Importing from sklearn**"],"metadata":{"id":"UcbX0o0XlHpX"}},{"cell_type":"code","source":["pca = PCA(n_components=1) \n","X_normalized = X_normalized.reshape(1,-1)\n","pca.fit(X_normalized)\n","X_reduced = pca.transform(X_normalized)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"5QtMHb_-rTy6","executionInfo":{"status":"ok","timestamp":1655016880142,"user_tz":-330,"elapsed":394,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}},"outputId":"2d3c6576-97b6-4dac-df0b-3e44415bd448"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_pca.py:499: RuntimeWarning: invalid value encountered in true_divide\n","  explained_variance_ = (S ** 2) / (n_samples - 1)\n"]}]},{"cell_type":"markdown","source":["### **Measuring Accuracy**"],"metadata":{"id":"iYZxHI7xlPXi"}},{"cell_type":"code","source":["Y_pred = DBNClassifier.predict(X_reduced)\n","print(Y_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":328},"id":"1FeP_46Wogx0","executionInfo":{"status":"error","timestamp":1655016909577,"user_tz":-330,"elapsed":684,"user":{"displayName":"20CE114 YAGNIK POSHIYA","userId":"11809518824151159275"}},"outputId":"9fe73dde-737c-4563-dbf7-ade02df249ec"},"execution_count":50,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-6aaa15b48959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDBNClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_reduced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-683e47cc4da7>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_network_format_to_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-683e47cc4da7>\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \"\"\"\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSupervisedDBNClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_output_units_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_proba_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-d341e4eae5cd>\u001b[0m in \u001b[0;36m_compute_output_units_matrix\u001b[0;34m(self, matrix_visible_units)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisible_units_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmatrix_visible_units\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mplaceholder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob_placeholders\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1157\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (1, 1) for Tensor 'Placeholder:0', which has shape '(?, 64)'"]}]}]}